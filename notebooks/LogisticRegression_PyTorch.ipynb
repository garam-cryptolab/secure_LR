{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import timeit\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score, log_loss, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "\n",
    "from tqdm import tqdm \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size = 1):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class CLDataSet(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.Tensor(x_data)\n",
    "        self.y_data = torch.Tensor(y_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_LIST = ['age','bmi','sex','emop','dept_DN','dept_GS','dept_OG','dept_OL','dept_OS','dept_PS','dept_UR','dept_nan','anetype_GA','anetype_NA','anetype_NB','anetype_nan','anetype_MAC','preop_pt','preop_aptt', 'preop_alb','preop_bun','preop_cr','preop_glu','preop_gpt','preop_got','preop_hb','preop_k','preop_na','preop_plt','preop_wbc']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path: str, hosid: str = 'amc', target = 'death30', var_list = VAR_LIST, impute: str = 'median') -> pd.DataFrame:  \n",
    "  df = pd.read_csv(data_path)\n",
    "  \n",
    "  # drop features\n",
    "  if hosid == 'snuh':\n",
    "    df.drop(columns=[\"hoslos\", \"opdur\",\"anedur\",\"op_code\",\"asa_1\",\"asa_2\",\"asa_3\",\"asa_4\",\"asa_5\",\"asa_nan\",\"iculos\",\"icu1\"],inplace=True)\n",
    "  elif hosid == 'eumc': \n",
    "    df.drop(columns=[\"hoslos\", \"opdur\",\"anedur\",\"op_code\",\"asa_1\",\"asa_2\",\"asa_3\",\"asa_4\",\"asa_5\",\"asa_nan\",\"iculos\",\"icu1\"],inplace=True)\n",
    "  elif hosid == 'brmh': \n",
    "    df.drop(columns=[\"hoslos\", \"opdur\",\"anedur\",\"op_code\",\"asa_1\",\"asa_2\",\"asa_3\",\"asa_4\",\"asa_5\",\"asa_nan\",\"iculos\",\"icu1\"],inplace=True)\n",
    "  \n",
    "  X = df[var_list]\n",
    "  y = df[target]\n",
    "  \n",
    "  # missing imputation \n",
    "  X.fillna(X.median(), inplace = True)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_LR(x_train, y_train, batch_size: int, epoch_size: int, learning_rate = .1, l1_ratio = 0.5, C = 0.1):\n",
    "\n",
    "    train_dataset = CLDataSet(x_train, y_train) \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size = batch_size,\n",
    "                              pin_memory=True)\n",
    "\n",
    "    input_size = 30 #$x_train.shape[1]\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = LogisticRegression(input_size, 1)#.to(device)\n",
    "\n",
    "    criterion = nn.BCELoss() #nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "     \n",
    "    lambda1 = l1_ratio \n",
    "    lambda2 = 1 - l1_ratio \n",
    "\n",
    "    # 4. 모델 학습\n",
    "    for epoch in range(epoch_size):\n",
    "        for data, target in train_loader: \n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Elastic Net Regularization 추가\n",
    "            l1 = 0\n",
    "            l2 = 0\n",
    "            for param in model.parameters():\n",
    "                l1 += torch.norm(param, 1)\n",
    "                l2 += torch.norm(param, 2)\n",
    "            loss = loss + lambda1 * l1 + lambda2 * l2\n",
    "            #l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            #l1_loss = l1_ratio * l1_norm\n",
    "            \n",
    "            #l2_norm = sum(p.pow(2.0).sum()\n",
    "            #            for p in model.parameters())\n",
    "            #l2_loss = (1 / C) * l2_norm\n",
    "\n",
    "            #loss = loss + l1_loss + l2_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, epoch_size, loss.item()))\n",
    "    \n",
    "    # save the model\n",
    "    #torch.save(model.state_dict(), 'saved_model.pth')\n",
    "    return model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_LR(model, x_test, y_test, batch_size):\n",
    "  # Set model to evaluation mode\n",
    "  model.eval()\n",
    "  \n",
    "  test_dataset = CLDataSet(x_test, y_test)\n",
    "  test_loader = DataLoader(test_dataset, \n",
    "                           batch_size = batch_size,\n",
    "                           shuffle=False,\n",
    "                           pin_memory=True)\n",
    "\n",
    "  # Make predictions on test set\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader: \n",
    "      output = model(data)\n",
    "      y_pred.extend(output.tolist())\n",
    "      y_true.extend(target.tolist())\n",
    "  \n",
    "  # Compute AUROC and AUPRC\n",
    "  auroc = roc_auc_score(y_true, y_pred)\n",
    "  auprc = average_precision_score(y_true, y_pred)\n",
    "  print(f\"AUROC: {auroc:.4f}, AUPRC: {auprc:.4f}\")\n",
    "  \n",
    "  # Compute ROC and PR curves\n",
    "  fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "  precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "  \n",
    "  # Plot ROC and PR curves\n",
    "  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "  \n",
    "  ax1.plot(fpr, tpr, label=f\"AUROC = {auroc:.4f}\")\n",
    "  ax1.plot([0, 1], [0, 1], \"k--\")\n",
    "  ax1.set_xlabel(\"False Positive Rate\")\n",
    "  ax1.set_ylabel(\"True Positive Rate\")\n",
    "  ax1.set_title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "  ax1.legend()\n",
    "  \n",
    "  ax2.plot(recall, precision, label=f\"AUPRC = {auprc:.4f}\")\n",
    "  ax2.set_xlabel(\"Recall\")\n",
    "  ax2.set_ylabel(\"Precision\")\n",
    "  ax2.set_title(\"Precision-Recall Curve\")\n",
    "  ax2.legend()\n",
    "  \n",
    "  plt.show()\n",
    "  \n",
    "  return auroc, auprc, precision, recall \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 120\n",
    "np.random.seed(random_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_auroc_list = []\n",
    "validation_auprc_list = []\n",
    "validation_loss_list = []\n",
    "\n",
    "num_bootstraps = 50\n",
    "sampling_size = 5000\n",
    "batch_size = 5000\n",
    "epoch_size = 300\n",
    "learning_rate = 0.05\n",
    "l1_ratio = 0.1\n",
    "C = 0.1\n",
    "optimizer = 'SGD'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_290770/3503248715.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.fillna(X.median(), inplace = True)\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "val_auroc = []\n",
    "val_auprc = []\n",
    "\n",
    "X, y = load_data(\"/data/abel.eo/data/amc/raw_data/210612_amc_light.csv\")\n",
    "\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "num_classes = len(np.unique(y))\n",
    "#test_size = len(y) - sampling_size\n",
    "#test_ratio = test_size / len(y)\n",
    "test_ratio = .2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size : (53217, 30)\n",
      "Epoch [10/300], Loss: 0.7373\n"
     ]
    }
   ],
   "source": [
    "for sample in range(num_bootstraps):\n",
    "  X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=test_ratio, shuffle=True, stratify=y)\n",
    "  print(f'X size : {X_train.shape}')\n",
    "  scaler = MinMaxScaler()\n",
    "\n",
    "  x_train = scaler.fit_transform(X_train).tolist()\n",
    "  y_train = scaler.fit_transform(Y_train.reshape(-1, 1)).tolist()\n",
    "  \n",
    "  x_test = scaler.fit_transform(X_test).tolist()\n",
    "  y_test = scaler.fit_transform(Y_test.reshape(-1, 1)).tolist()\n",
    "  \n",
    "  #x_train = torch.tensor(x_train, dtype = torch.float)\n",
    "  #x_test = torch.tensor(x_test, dtype = torch.float)\n",
    "  #y_train = torch.tensor(y_train, dtype= torch.float)\n",
    "  #y_test = torch.tensor(y_test, dtype = torch.float)\n",
    "  #y_test = torch.tensor(y_test.reshape(-1, 1), dtype = torch.float)\n",
    "  \n",
    "  #train logistic regression \n",
    "  model, sample_loss = train_LR(x_train, y_train, \n",
    "                                batch_size, \n",
    "                                epoch_size, \n",
    "                                learning_rate, \n",
    "                                l1_ratio, \n",
    "                                C)\n",
    "  train_loss.append(sample_loss.cpu().detach().numpy())\n",
    "\n",
    "  sample_auroc, sample_auprc, _, _ = test_LR(model, x_test, y_test, batch_size)\n",
    "  val_auroc.append(sample_auroc)#.detach().numpy())\n",
    "  val_auprc.append(sample_auprc)#.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100, 15000, 0.7185607, 0.6356524994349431, 0.004016987266158922)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_size, epoch_size, batch_size, np.median(train_loss), np.median(val_auroc), np.median(val_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300, 1024, 0.72877204, 0.48196816331785713, 0.013489136200691197)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_size, epoch_size, batch_size, np.median(train_loss), np.median(val_auroc), np.median(val_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300, 2048, 0.25735503, 0.5382600027548575, 0.01272536218042938)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_size, epoch_size, batch_size, np.median(train_loss), np.median(val_auroc), np.median(val_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300, 5000, 0.7050331, 0.4283803780944674, 0.008402691838680101)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_size, epoch_size, batch_size, np.median(train_loss), np.median(val_auroc), np.median(val_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 120, 10000, 1.3223097, 0.43399444035765056, 0.011111961742432973)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_size, epoch_size, batch_size, np.median(train_loss), np.median(val_auroc), np.median(val_auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDK-MohvCbfc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
